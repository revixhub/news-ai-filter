import asyncio
from datetime import datetime, timedelta
from typing import List, Optional
import structlog

from src.database import DatabaseManager
from src.models import Source, SourceType, RawContent, ParsedContent
from src.collectors.telegram_collector import TelegramCollector
from src.collectors.web_collector import WebCollector
from src.collectors.youtube_collector import YouTubeCollector
from src.analyzers.ai_analyzer import AIAnalyzer
from src.digest.digest_generator import DigestGenerator
import config

logger = structlog.get_logger()

class DigestService:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤"""
    
    def __init__(self):
        self.db = DatabaseManager(config.DATABASE_PATH)
        self.ai_analyzer = AIAnalyzer()
        self.digest_generator = DigestGenerator()
        self._init_default_sources()
    
    def _init_default_sources(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        default_sources = [
            # Telegram –∫–∞–Ω–∞–ª—ã
            Source(None, SourceType.TELEGRAM, "–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥", "@marketing_news", True),
            Source(None, SourceType.TELEGRAM, "Digital Agency", "@digitalagency", True),
            Source(None, SourceType.TELEGRAM, "Sostav", "@sostav", True),
            
            # –í–µ–±-—Å–∞–π—Ç—ã
            Source(None, SourceType.WEBSITE, "Sostav.ru", "https://sostav.ru/rss/", True),
            Source(None, SourceType.WEBSITE, "VC.ru Marketing", "https://vc.ru/marketing/rss", True),
            Source(None, SourceType.WEBSITE, "Cossa", "https://www.cossa.ru/rss/", True),
            
            # YouTube –∫–∞–Ω–∞–ª—ã
            Source(None, SourceType.YOUTUBE, "–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ –¢–í", "UCMarketingTV", True),
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –µ—Å–ª–∏ –∏—Ö –µ—â–µ –Ω–µ—Ç
        existing_sources = self.db.get_active_sources()
        if not existing_sources:
            for source in default_sources:
                try:
                    self.db.add_source(source)
                    logger.info("Added default source", name=source.name, type=source.type.value)
                except Exception as e:
                    logger.warning("Failed to add default source", name=source.name, error=str(e))
    
    async def generate_daily_digest(self, user_id: int) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
        start_time = datetime.now()
        
        try:
            logger.info("Starting digest generation", user_id=user_id)
            
            # 1. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            raw_content = await self._collect_from_all_sources()
            logger.info("Raw content collected", count=len(raw_content))
            
            if not raw_content:
                return self._generate_empty_digest()
            
            # 2. –ü–∞—Ä—Å–∏–Ω–≥ –∏ –æ—á–∏—Å—Ç–∫–∞
            parsed_content = await self._parse_and_filter_content(raw_content)
            logger.info("Content parsed", count=len(parsed_content))
            
            if not parsed_content:
                return self._generate_empty_digest()
            
            # 3. AI –∞–Ω–∞–ª–∏–∑
            analyzed_content = await self.ai_analyzer.analyze_batch(parsed_content)
            logger.info("Content analyzed", count=len(analyzed_content))
            
            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
            self._save_analyzed_content(analyzed_content)
            
            # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤
            top_content = sorted(analyzed_content, key=lambda x: x.importance_score, reverse=True)[:5]
            insights = await self.ai_analyzer.generate_insights(top_content)
            
            # 6. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–∞
            digest = self.digest_generator.generate_digest(analyzed_content, insights, user_id)
            
            # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–∞
            digest_id = self.db.save_digest(digest)
            digest.id = digest_id
            
            # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
            self.digest_generator.save_to_file(digest)
            
            # 9. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Telegram
            telegram_text = self.digest_generator.format_for_telegram(digest)
            
            # 10. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            processing_time = (datetime.now() - start_time).total_seconds()
            self._save_metrics(digest_id, processing_time, len(raw_content), len(analyzed_content))
            
            logger.info("Digest generation completed", 
                       user_id=user_id, 
                       processing_time=processing_time,
                       items_count=len(analyzed_content))
            
            return telegram_text
            
        except Exception as e:
            logger.error("Digest generation failed", user_id=user_id, error=str(e))
            return None
    
    async def _collect_from_all_sources(self) -> List[RawContent]:
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        sources = self.db.get_active_sources()
        all_content = []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        telegram_sources = [s for s in sources if s.type == SourceType.TELEGRAM]
        web_sources = [s for s in sources if s.type == SourceType.WEBSITE]
        youtube_sources = [s for s in sources if s.type == SourceType.YOUTUBE]
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = []
        
        # Telegram
        for source in telegram_sources:
            collector = TelegramCollector(source)
            tasks.append(self._safe_collect(collector, source.name))
        
        # Web
        for source in web_sources:
            collector = WebCollector(source)
            tasks.append(self._safe_collect(collector, source.name))
        
        # YouTube
        for source in youtube_sources:
            collector = YouTubeCollector(source)
            tasks.append(self._safe_collect(collector, source.name))
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for result in results:
            if isinstance(result, list):
                all_content.extend(result)
            elif isinstance(result, Exception):
                logger.error("Collection task failed", error=str(result))
        
        return all_content
    
    async def _safe_collect(self, collector, source_name: str) -> List[RawContent]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–±–æ—Ä —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            return await collector.collect()
        except Exception as e:
            logger.error("Collection failed", source=source_name, error=str(e))
            return []
    
    async def _parse_and_filter_content(self, raw_content: List[RawContent]) -> List[ParsedContent]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        parsed_items = []
        
        for item in raw_content:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
            if self.db.check_duplicate(item.title, item.source_id):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–∫–ª–∞–º–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            parsed_item = ParsedContent(
                id=None,
                source_id=item.source_id,
                source_name=item.source_name,
                title=item.title,
                content=item.content,
                url=item.url,
                published_at=item.published_at,
                processed_at=datetime.now(),
                is_duplicate=False,
                is_ad=await self.ai_analyzer.check_for_ad_content(
                    ParsedContent(None, item.source_id, item.source_name, 
                                item.title, item.content, item.url, item.published_at)
                )
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            content_id = self.db.save_content(parsed_item)
            parsed_item.id = content_id
            
            parsed_items.append(parsed_item)
        
        return parsed_items
    
    def _save_analyzed_content(self, analyzed_content: List):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î"""
        for item in analyzed_content:
            if item.id:
                self.db.update_content_analysis(
                    item.id, 
                    item.importance_score, 
                    item.category.value,
                    item.explanation
                )
    
    def _save_metrics(self, digest_id: Optional[int], processing_time: float, 
                     raw_count: int, processed_count: int):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        from src.models import ProcessingMetrics
        
        metrics = ProcessingMetrics(
            digest_id=digest_id,
            processing_time=processing_time,
            sources_count=len(self.db.get_active_sources()),
            raw_items_count=raw_count,
            processed_items_count=processed_count,
            top_items_count=min(5, processed_count),
            errors_count=0,  # TODO: –ø–æ–¥—Å—á–µ—Ç –æ—à–∏–±–æ–∫
            created_at=datetime.now()
        )
        
        self.db.save_metrics(metrics)
    
    def _generate_empty_digest(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—É—Å—Ç–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
        return """
ü§ñ *–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ –î–∞–π–¥–∂–µ—Å—Ç ‚Äî {}*

üòî –°–µ–≥–æ–¥–Ω—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö –≤–∞–∂–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π\\. 

–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:
‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
‚Ä¢ –ù–µ—Ç –Ω–æ–≤—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
‚Ä¢ –í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–∫–∞–∑–∞–ª–∏—Å—å –Ω–µ–≤–∞–∂–Ω—ã–º–∏ \\(< 30 –±–∞–ª–ª–æ–≤\\)

üîÑ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /digest —á–µ—Ä–µ–∑ —á–∞—Å –∏–ª–∏ –∑–∞–≤—Ç—Ä–∞ —É—Ç—Ä–æ–º\\!
        """.format(datetime.now().strftime('%d\\.%m\\.%Y'))
    
    async def get_sources_info(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
        sources = self.db.get_active_sources()
        
        if not sources:
            return "‚ùå –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
        
        telegram_sources = [s for s in sources if s.type == SourceType.TELEGRAM]
        web_sources = [s for s in sources if s.type == SourceType.WEBSITE]
        youtube_sources = [s for s in sources if s.type == SourceType.YOUTUBE]
        
        text = f"üìã *–ê–∫—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ \\({len(sources)} —à—Ç\\.\\):*\n\n"
        
        if telegram_sources:
            text += "üì± *Telegram –∫–∞–Ω–∞–ª—ã:*\n"
            for source in telegram_sources:
                text += f"  ‚Ä¢ {self._escape_markdown(source.name)}\n"
            text += "\n"
        
        if web_sources:
            text += "üåê *–í–µ–±\\-—Å–∞–π—Ç—ã:*\n"
            for source in web_sources:
                text += f"  ‚Ä¢ {self._escape_markdown(source.name)}\n"
            text += "\n"
        
        if youtube_sources:
            text += "üì∫ *YouTube –∫–∞–Ω–∞–ª—ã:*\n"
            for source in youtube_sources:
                text += f"  ‚Ä¢ {self._escape_markdown(source.name)}\n"
            text += "\n"
        
        text += f"üîÑ –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–±–æ—Ä: —Å–µ–≥–æ–¥–Ω—è –≤ {config.DIGEST_TIME}\n"
        text += "‚è∞ –°–ª–µ–¥—É—é—â–∏–π —Å–±–æ—Ä: –∑–∞–≤—Ç—Ä–∞ –≤ —Ç–æ –∂–µ –≤—Ä–µ–º—è"
        
        return text
    
    async def get_stats(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
            recent_content = self.db.get_recent_content(hours=24*7)
            
            if not recent_content:
                return "üìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:* –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π"
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–Ω—è–º
            by_day = {}
            for item in recent_content:
                day = item.published_at.strftime('%Y-%m-%d')
                if day not in by_day:
                    by_day[day] = 0
                by_day[day] += 1
            
            avg_per_day = len(recent_content) / 7
            
            text = "üìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ 7 –¥–Ω–µ–π:*\n\n"
            text += f"üìà –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(recent_content)} –Ω–æ–≤–æ—Å—Ç–µ–π\n"
            text += f"üìä –í —Å—Ä–µ–¥–Ω–µ–º –∑–∞ –¥–µ–Ω—å: {avg_per_day:.1f} –Ω–æ–≤–æ—Å—Ç–µ–π\n"
            text += f"üéØ –ê–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.db.get_active_sources())}\n\n"
            
            text += "*–ü–æ –¥–Ω—è–º:*\n"
            for day, count in sorted(by_day.items())[-3:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è
                date_formatted = datetime.strptime(day, '%Y-%m-%d').strftime('%d\\.%m')
                text += f"  ‚Ä¢ {date_formatted}: {count} –Ω–æ–≤–æ—Å—Ç–µ–π\n"
            
            return text
            
        except Exception as e:
            logger.error("Stats generation failed", error=str(e))
            return "‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"
    
    def _escape_markdown(self, text: str) -> str:
        """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ markdown —Å–∏–º–≤–æ–ª–æ–≤"""
        escape_chars = ['_', '*', '[', ']', '(', ')', '~', '`', '>', '#', '+', '-', '=', '|', '{', '}', '.', '!']
        for char in escape_chars:
            text = text.replace(char, f'\\{char}')
        return text 